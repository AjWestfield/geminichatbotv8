import { GoogleGenerativeAI } from "@google/generative-ai"
import { MCPToolsContext } from "@/lib/mcp/mcp-tools-context"
import { MCPServerIntelligence } from "@/lib/mcp/mcp-server-intelligence"
import { MCPServerManager } from "@/lib/mcp/mcp-server-manager"
import { MCPConfigManager } from "@/lib/mcp/mcp-config-manager"
import { MCPGitHubPrompts } from "@/lib/mcp/mcp-github-prompts"
import { MCP_AGENT_INSTRUCTIONS_ENHANCED, MCP_SYSTEM_PROMPT_ENHANCED } from "@/lib/mcp/mcp-agent-instructions-enhanced"
import { VideoGenerationHandler } from "@/lib/video-generation-handler"
import { ImageEditingHandler } from "@/lib/image-editing-handler"
import { ImageGenerationHandler } from "@/lib/image-generation-handler"
import { ImageUrlConverter } from "@/lib/image-url-converter"
import { streamText } from 'ai'
import { convertToCoreMessages, type Message } from 'ai'
import { ToolOrchestrator, OrchestrationContext } from "@/lib/tool-orchestrator"

// Initialize Gemini AI
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || "")

// Note: MCP servers will be auto-connected when needed by MCPToolsContext.getAvailableTools()

// Legacy TTS detection functions - these are being phased out in favor of tool orchestrator
const containsTTSCommand = (content: string): boolean => {
  return /\b(tts|text.?to.?speech|read.*aloud|voice.*over|narrat)\b/i.test(content);
};

const containsMultiSpeakerTTSCommand = (content: string): boolean => {
  return /\b(multi.?speaker|dialogue|conversation|multiple.?voice)\b/i.test(content);
};

const extractTTSContent = (content: string): any => {
  return {
    text: content,
    voiceName: 'default',
    style: 'natural',
    multiSpeaker: false,
    generateScript: false
  };
};

const processMultiSpeakerTTS = async (content: string, messages: any[]): Promise<any> => {
  // This is a stub - actual implementation would call the TTS API
  return {
    success: false,
    error: 'Multi-speaker TTS not implemented in legacy mode'
  };
};

// Stub functions for tool orchestrator handlers
const handleMultiSpeakerTTS = async (content: string, messages: any[], parameters: any): Promise<any> => {
  // Stub implementation
  return null;
};

const handleSingleSpeakerTTS = async (content: string, messages: any[], parameters: any): Promise<any> => {
  // Stub implementation
  return null;
};

const generateMultiSpeakerResponse = (ttsData: any): string => {
  return 'Multi-speaker TTS response';
};

const generateSingleSpeakerResponse = (ttsData: any): string => {
  return 'Single-speaker TTS response';
};

const handleImageGeneration = async (request: any): Promise<any> => {
  // Stub implementation
  return null;
};

const handleVideoGeneration = async (request: any): Promise<any> => {
  // Stub implementation
  return null;
};

const handleImageEditing = async (request: any): Promise<any> => {
  // Stub implementation
  return null;
};

export async function POST(req: Request) {
  try {
    // Parse request
    const { messages, model = "gemini-2.5-pro-preview-06-05", fileUri, fileMimeType, transcription, multipleFiles, imageGenerationModel, imageEditingModel, imageSettings } = await req.json()

    console.log(`[Chat API] Request received with model: ${model}`);
    console.log(`[Chat API] Model type: ${typeof model}`);
    console.log(`[Chat API] Available models: gemini-2.5-pro-preview-06-05, gemini-2.5-flash-preview-05-20, gemini-2.0-flash-exp, Claude Sonnet 4`);

    if (!messages || !Array.isArray(messages)) {
      return new Response("Invalid messages format", { status: 400 })
    }

    // Check if it's a Claude model
    if (model === "Claude Sonnet 4") {
      console.log('[Chat API] Routing to Claude Sonnet 4 handler');
      // Import Claude handler dynamically to avoid circular dependencies
      const { handleClaudeRequest } = await import('./claude-handler');
      return handleClaudeRequest(messages);
    }

    // Build system instruction - use only MCP_AGENT_INSTRUCTIONS_ENHANCED to avoid duplication
    const currentDate = new Date().toLocaleDateString('en-US', { 
      weekday: 'long', 
      year: 'numeric', 
      month: 'long', 
      day: 'numeric' 
    });
    const systemInstruction = `Today's date is ${currentDate}. When users ask for current information, news, or recent events, remember to use this date context.\n\n${MCP_AGENT_INSTRUCTIONS_ENHANCED}`

    // Get the generative model with appropriate settings
    const modelConfig: any = {
      model,
      systemInstruction
    }

    // Add specific configurations for video-capable models
    if (model === "gemini-2.0-flash-exp" && fileMimeType?.startsWith("video/")) {
      // Gemini 2.0 Flash might have different video capabilities
      modelConfig.generationConfig = {
        temperature: 0.4, // Lower temperature for more focused analysis
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 8192,
      }
    }

    console.log(`[Chat API] Using Gemini model: ${model}`);
    console.log(`[Chat API] Model config:`, JSON.stringify(modelConfig, null, 2));
    const generativeModel = genAI.getGenerativeModel(modelConfig)

    // Get the last user message
    const lastMessage = messages[messages.length - 1]
    if (!lastMessage || lastMessage.role !== "user") {
      return new Response("No user message found", { status: 400 })
    }

    // Convert messages to Gemini format, excluding system messages and initial assistant greeting
    const history = messages
      .filter((m) => m.role !== "system" && m.id !== "welcome-message")
      .slice(0, -1) // Exclude the last message which we'll send separately
      .map((m) => ({
        role: m.role === "user" ? "user" : "model",
        parts: [{ text: m.content }],
      }))

    // Ensure history starts with user message or is empty
    if (history.length > 0 && history[0].role !== "user") {
      history.shift() // Remove first message if it's not from user
    }

    // Start chat session with history
    // Use higher token limits for video analysis
    const isVideoAnalysis = fileMimeType?.startsWith("video/")
    const chat = generativeModel.startChat({
      history,
      generationConfig: {
        temperature: 0.7,
        topK: 1,
        topP: 1,
        maxOutputTokens: isVideoAnalysis ? 8192 : 4096, // Much higher limit for videos
      },
    })

    // Prepare content parts for multimodal input
    const contentParts = []

    // Add multiple files if provided
    if (multipleFiles && multipleFiles.length > 0) {
      console.log(`Processing ${multipleFiles.length} files`)
      for (const file of multipleFiles) {
        if (file.uri && file.mimeType) {
          console.log(`Adding file: ${file.name}, ${file.mimeType}`)
          contentParts.push({
            fileData: {
              mimeType: file.mimeType,
              fileUri: file.uri
            }
          })
        }
      }
    } 
    // Add single file if provided (fallback for backward compatibility)
    else if (fileUri && fileMimeType) {
      console.log(`Processing file: ${fileMimeType}, URI: ${fileUri}`)
      contentParts.push({
        fileData: {
          mimeType: fileMimeType,
          fileUri: fileUri
        }
      })
    }

    // Prepare the message content
    let messageContent = lastMessage.content

    // If it's a media file and no user input, add appropriate analysis instruction
    const hasMultipleFiles = multipleFiles && multipleFiles.length > 0
    if ((fileMimeType || hasMultipleFiles) && !lastMessage.content.trim()) {
      console.log(`[Chat API] Detected media file(s) with no text`)
      console.log(`[Chat API] Message content is empty: "${lastMessage.content}"`)
      console.log(`[Chat API] Has multiple files: ${hasMultipleFiles}, count: ${multipleFiles?.length || 0}`)
      
      if (hasMultipleFiles) {
        // For multiple files, analyze all of them
        const imageCount = multipleFiles.filter((f: { mimeType?: string }) => f.mimeType?.startsWith("image/")).length
        const videoCount = multipleFiles.filter((f: { mimeType?: string }) => f.mimeType?.startsWith("video/")).length
        const audioCount = multipleFiles.filter((f: { mimeType?: string }) => f.mimeType?.startsWith("audio/")).length
        
        if (imageCount > 0) {
          messageContent = `Please analyze all ${multipleFiles.length} uploaded files. For each image, provide:
1. A detailed description of what you see
2. Key elements, objects, people, or text visible
3. The mood, style, or atmosphere
4. Any notable technical aspects (composition, lighting, etc.)
5. How the images relate to each other (if applicable)

Please analyze each file thoroughly and provide insights.`
        } else {
          messageContent = "Please analyze all the uploaded files and provide detailed insights about their content."
        }
      } else if (fileMimeType) {
        // Single file handling
        if (fileMimeType.startsWith("audio/")) {
          messageContent = `Please analyze this audio file and provide:

1. **Complete Transcription**: 
   - Transcribe ALL spoken words, dialogue, or narration word-for-word
   - Use timestamps to indicate when different speakers talk (e.g., [00:15] Speaker A: "...")
   - Identify different speakers as Speaker A, Speaker B, etc.
   - Note the language being spoken

2. **Audio Analysis**:
   - Describe any background music, sound effects, or ambient sounds
   - Note the tone, mood, and pacing of the audio
   - Identify the type of recording (podcast, music, interview, etc.)

3. **Technical Details**:
   - Audio quality and clarity
   - Estimated duration
   - Any notable audio characteristics

IMPORTANT: Please provide a COMPLETE word-for-word transcription of all spoken content. Do not summarize - transcribe exactly what is said.`
        } else if (fileMimeType.startsWith("video/")) {
          messageContent = `Please provide a comprehensive analysis of this entire video from start to finish. Include:

1. **Overview**: Brief summary of the video's purpose and content
2. **Visual Analysis**:
   - Describe all scenes in chronological order with timestamps
   - Note any text, graphics, or visual effects
   - Describe camera movements, transitions, and editing style
3. **Audio Transcription & Analysis**:
   - Provide a complete transcription of ALL spoken words, narration, or dialogue
   - Use timestamps to indicate when different speakers talk (e.g., [00:15] Speaker A: "...")
   - Identify different speakers as Speaker A, Speaker B, etc.
   - Note the language being spoken
   - Describe background music, sound effects, or audio cues
   - Note the tone and pacing of audio elements
4. **Technical Aspects**: Video quality, aspect ratio, production value
5. **Complete Timeline**: Provide a scene-by-scene breakdown with timestamps
6. **Key Messages**: Main themes or messages conveyed
7. **Overall Assessment**: Style, effectiveness, and notable features

IMPORTANT: Please transcribe ALL audio content word-for-word. Do not summarize or paraphrase the spoken content - provide the exact words spoken.

Please ensure you analyze the ENTIRE video duration from beginning to end.`
        } else if (fileMimeType.startsWith("image/")) {
          console.log(`[Chat API] Detected single image upload with no text - returning IMAGE_OPTIONS`)
          // Return image options instead of auto-analyzing for single images
          messageContent = "[IMAGE_OPTIONS]" + JSON.stringify({
            type: "image_upload",
            fileUri: fileUri,
            options: [
              {
                id: "analyze",
                label: "ðŸ” Analyze Image",
                description: "Get detailed insights about the image content"
              },
            {
              id: "edit", 
              label: "âœï¸ Edit Image",
              description: "Modify the image using AI-powered editing"
            },
            {
              id: "animate",
              label: "ðŸŽ¬ Animate Image", 
              description: "Transform this image into a video"
            }
          ]
        }) + "[/IMAGE_OPTIONS]";
        console.log(`[Chat API] Set messageContent to include IMAGE_OPTIONS marker`)
        }
      }
    } else if (fileMimeType?.startsWith("video/") && lastMessage.content.trim().toLowerCase().includes("analyze")) {
      // If user asks to analyze a video, ensure comprehensive analysis with transcription
      messageContent = lastMessage.content + `

Please ensure your analysis includes:
- A COMPLETE word-for-word transcription of ALL spoken content with timestamps
- The complete video from start to finish
- All visual scenes with timestamps
- Technical aspects and production quality
- A detailed timeline of events
- Key messages and themes

IMPORTANT: Transcribe all audio content exactly as spoken. Use timestamps to indicate when different speakers talk.

Analyze the ENTIRE video duration, not just the beginning.`
    }


    // Add text message
    let finalMessageContent = messageContent

    // Check if this is a follow-up question that should force web search
    const isFollowUpSearch = messageContent.startsWith('[FORCE_WEB_SEARCH]')
    if (isFollowUpSearch) {
      finalMessageContent = messageContent.replace('[FORCE_WEB_SEARCH]', '').trim()
    }

    // Check if web search is needed using Perplexity
    const { SearchIntentDetector } = await import('@/lib/search-intent-detector');
    const detector = new SearchIntentDetector();
    const searchIntent = detector.detectSearchIntent(finalMessageContent);
    
    // If web search is needed OR it's a follow-up question, route to Perplexity
    if ((searchIntent.needsSearch || isFollowUpSearch) && !fileUri && !fileMimeType && !multipleFiles) {
      console.log('[Chat API] Web search detected, routing to Perplexity');
      console.log('[Chat API] Search intent:', searchIntent);
      console.log('[Chat API] Is follow-up search:', isFollowUpSearch);
      
      try {
        // Import Perplexity client and prepare for streaming
        const { PerplexityClient } = await import('@/lib/perplexity-client');
        const client = new PerplexityClient();

        // Prepare system message with current date
        const systemMessage = {
          role: 'system' as const,
          content: `You are a helpful AI assistant with access to real-time web search. 
            Today's date is ${new Date().toLocaleDateString('en-US', { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' })}.
            Always provide the most current and up-to-date information based on search results.
            Always cite your sources when using searched information. 
            Provide accurate, up-to-date information based on search results.
            Format citations as [Source Name](URL) when referencing search results.`
        };
        
        // Get last user message
        const lastUserMessage = messages[messages.length - 1];
        const perplexityMessages = [
          systemMessage,
          {
            role: lastUserMessage.role as 'user',
            content: lastUserMessage.content
          }
        ];

        // Prepare search options
        const searchOptions: any = {
          search_mode: 'web',
          return_images: true,
          return_related_questions: true
        };

        if (searchIntent.timeFilter) {
          searchOptions.search_recency_filter = searchIntent.timeFilter;
        }

        if (searchIntent.domainFilter) {
          searchOptions.search_domain_filter = searchIntent.domainFilter;
        }

        // Create a transform stream to handle Perplexity's streaming format
        const encoder = new TextEncoder();
        const decoder = new TextDecoder();
        let buffer = '';
        let searchMetadata: any = {
          hasSearchResults: true,
          searchResults: [],
          images: [],
          followUpQuestions: []
        };
        let isFirstChunk = true;

        const stream = new ReadableStream({
          async start(controller) {
            let isControllerClosed = false;
            
            // Safe wrapper functions
            const safeEnqueue = (data: Uint8Array) => {
              try {
                if (!isControllerClosed && controller.desiredSize !== null) {
                  controller.enqueue(data);
                }
              } catch (error: any) {
                if (!error.message?.includes('Controller is already closed')) {
                  console.error('[Chat API] Enqueue error:', error);
                }
              }
            };
            
            const safeClose = () => {
              try {
                if (!isControllerClosed) {
                  isControllerClosed = true;
                  controller.close();
                }
              } catch (error) {
                // Already closed
              }
            };
            
            const safeError = (error: any) => {
              try {
                if (!isControllerClosed) {
                  isControllerClosed = true;
                  controller.error(error);
                }
              } catch (e) {
                // Already closed
              }
            };
            
            try {
              // Send initial "searching" indicator
              safeEnqueue(encoder.encode(`0:${JSON.stringify('[SEARCHING_WEB]')}\n`));

              await client.streamSearch(
                perplexityMessages,
                searchOptions,
                (chunk: string) => {
                  buffer += chunk;
                  const lines = buffer.split('\n');
                  buffer = lines.pop() || '';

                  for (const line of lines) {
                    if (line.trim().startsWith('data: ')) {
                      const data = line.slice(6);
                      if (data === '[DONE]') {
                        // Stream is done, metadata will be sent below
                        return;
                      }

                      try {
                        const parsed = JSON.parse(data);
                        
                        // Extract content from the delta
                        if (parsed.choices?.[0]?.delta?.content) {
                          const content = parsed.choices[0].delta.content;
                          safeEnqueue(encoder.encode(`0:${JSON.stringify(content)}\n`));
                        }

                        // Extract citations and search results from the full message (usually in the last chunk)
                        if (parsed.choices?.[0]?.message) {
                          const message = parsed.choices[0].message;
                          if (parsed.citations) {
                            searchMetadata.searchResults = parsed.citations.map((citation: any, index: number) => ({
                              title: citation.title || `Source ${index + 1}`,
                              url: citation.url || citation,
                              date: citation.date,
                              thumbnail: citation.thumbnail || citation.image,
                              description: citation.snippet || citation.description
                            }));
                          }
                          if (parsed.images) {
                            searchMetadata.images = parsed.images;
                          }
                          if (parsed.related_questions) {
                            searchMetadata.followUpQuestions = parsed.related_questions;
                          }
                        }

                        // Also check for search_results and images at the top level
                        if (parsed.search_results) {
                          searchMetadata.searchResults = parsed.search_results;
                        }
                        if (parsed.images) {
                          searchMetadata.images = parsed.images;
                        }
                        if (parsed.related_questions) {
                          searchMetadata.followUpQuestions = parsed.related_questions;
                        }
                        
                        // Debug logging
                        if (parsed.citations || parsed.search_results || parsed.images || parsed.related_questions) {
                          console.log('[Chat API] Found metadata in stream:', {
                            hasCitations: !!parsed.citations,
                            hasSearchResults: !!parsed.search_results,
                            hasImages: !!parsed.images,
                            imagesCount: parsed.images?.length || 0,
                            hasRelatedQuestions: !!parsed.related_questions,
                            relatedQuestionsCount: parsed.related_questions?.length || 0
                          });
                        }
                      } catch (e) {
                        console.error('[Chat API] Error parsing Perplexity chunk:', e);
                      }
                    }
                  }
                }
              );

              // Process any remaining buffer
              if (buffer.trim()) {
                safeEnqueue(encoder.encode(`0:${JSON.stringify(buffer)}\n`));
              }

              // Generate follow-up questions based on the search query
              const generateFollowUpQuestions = (query: string, searchResults: any[]) => {
                const questions: string[] = [];
                
                // Extract key terms from the query
                const queryLower = query.toLowerCase();
                
                // Generate more specific, contextual follow-up questions
                if (queryLower.includes('tesla') || queryLower.includes('car') || queryLower.includes('vehicle')) {
                  // Extract specific model or aspect mentioned
                  const hasModel3 = queryLower.includes('model 3');
                  const hasModelY = queryLower.includes('model y');
                  const hasCybertruck = queryLower.includes('cybertruck');
                  
                  if (hasModel3) {
                    questions.push(
                      "What's the latest Model 3 Performance specs and pricing?",
                      "How does Model 3 compare to competing EVs in 2025?",
                      "What are the new features in the 2025 Model 3 refresh?",
                      "Is there a Model 3 refresh coming soon?"
                    );
                  } else if (hasModelY) {
                    questions.push(
                      "What's new in the 2025 Model Y update?",
                      "How does Model Y seating configuration work?",
                      "What's the real-world range of Model Y Long Range?",
                      "Are there any Model Y production delays?"
                    );
                  } else if (hasCybertruck) {
                    questions.push(
                      "What's the latest Cybertruck delivery timeline?",
                      "How much does the Cybertruck actually cost?",
                      "What are real Cybertruck owner reviews saying?",
                      "Is the Cybertruck available in all states?"
                    );
                  } else if (queryLower.includes('news') || queryLower.includes('latest')) {
                    questions.push(
                      "What's Tesla's latest stock performance?",
                      "Are there any new Tesla factories being built?",
                      "What's the status of Tesla FSD (Full Self-Driving)?",
                      "What are Tesla's main competitors doing?"
                    );
                  } else {
                    questions.push(
                      "What are Tesla's upcoming models?",
                      "How does Tesla's charging network compare to others?",
                      "What's the average wait time for Tesla delivery?",
                      "Are there Tesla tax credits available in 2025?"
                    );
                  }
                } else if (queryLower.includes('plane crash') || queryLower.includes('air india') || queryLower.includes('aviation')) {
                  questions.push(
                    "What caused the Air India plane crash?",
                    "Were there any survivors from the crash?",
                    "What safety measures are being implemented?",
                    "Has the airline issued an official statement?"
                  );
                } else if (queryLower.includes('nba') || queryLower.includes('basketball')) {
                  questions.push(
                    "Who's leading the NBA MVP race this season?",
                    "What are the latest NBA playoff standings?",
                    "Which NBA trades happened this week?",
                    "What are the biggest NBA injuries right now?"
                  );
                } else if (queryLower.includes('nfl') || queryLower.includes('football')) {
                  questions.push(
                    "What are the current NFL playoff scenarios?",
                    "Who are the top NFL MVP candidates?",
                    "What's the latest NFL injury report?",
                    "Which NFL teams clinched playoff spots?"
                  );
                } else if (queryLower.includes('ai') || queryLower.includes('artificial intelligence')) {
                  questions.push(
                    "What are the latest AI breakthroughs this month?",
                    "How are companies using AI in production?",
                    "What are the new AI regulations being proposed?",
                    "Which AI startups just got funding?"
                  );
                } else if (queryLower.includes('crypto') || queryLower.includes('bitcoin') || queryLower.includes('ethereum')) {
                  questions.push(
                    "What's driving the current crypto market trend?",
                    "Are there new crypto regulations coming?",
                    "Which cryptocurrencies are gaining adoption?",
                    "What are institutional investors doing with crypto?"
                  );
                } else if (queryLower.includes('stock') || queryLower.includes('market')) {
                  questions.push(
                    "What are the top performing stocks today?",
                    "How is the Federal Reserve affecting markets?",
                    "What sectors are analysts most bullish on?",
                    "Are we heading for a market correction?"
                  );
                } else if (queryLower.includes('technology') || queryLower.includes('tech')) {
                  questions.push(
                    "What are the breakthrough technologies of 2025?",
                    "Which tech companies are hiring the most?",
                    "What's the latest in quantum computing?",
                    "How is tech addressing climate change?"
                  );
                } else {
                  // More intelligent generic questions based on the query structure
                  const queryWords = query.split(' ').filter(word => word.length > 3);
                  const mainTopic = queryWords.find(word => !['what', 'when', 'where', 'how', 'latest', 'news'].includes(word.toLowerCase())) || 'this topic';
                  
                  questions.push(
                    `What are experts predicting about ${mainTopic}?`,
                    `How does ${mainTopic} compare globally?`,
                    `What's the timeline for ${mainTopic} developments?`,
                    `Who are the key players in ${mainTopic}?`
                  );
                }
                
                return questions.slice(0, 4); // Return max 4 questions
              };

              // Add follow-up questions to metadata
              if (!searchMetadata.followUpQuestions || searchMetadata.followUpQuestions.length === 0) {
                searchMetadata.followUpQuestions = generateFollowUpQuestions(
                  lastUserMessage.content,
                  searchMetadata.searchResults
                );
              }

              // Always send metadata at the end
              console.log('[Chat API] Sending search metadata:', searchMetadata);
              const metadataStr = `\n\n[SEARCH_METADATA]${JSON.stringify(searchMetadata)}[/SEARCH_METADATA]`;
              safeEnqueue(encoder.encode(`0:${JSON.stringify(metadataStr)}\n`));
              safeEnqueue(encoder.encode(`d:{"finishReason":"stop"}\n`));

              safeClose();
            } catch (error) {
              console.error('[Chat API] Perplexity streaming error:', error);
              safeError(error);
            }
          }
        });

        return new Response(stream, {
          headers: {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
          },
        });
      } catch (error) {
        console.error('[Chat API] Perplexity search error:', error);
        // Fall back to regular Gemini response
      }
    }

    // Check if this is an API key response
    if (messageContent.includes('API_KEY_PROVIDED:')) {
      console.log('[Chat API] Detected API key response')
      // The agent will handle this internally, just pass it through
    }

    // Check if this is an MCP server management request
    const mcpServerPatterns = [
      /(?:add|install|setup|configure|enable|remove|delete|list|show)\s+.*mcp\s+server/i,
      /mcp\s+server.*(?:add|install|setup|configure|enable|remove|delete|list|show)/i,
      /(?:add|install|setup|configure)\s+(?:the\s+)?(?:@[\w/-]+|[\w-]+)\s+(?:mcp\s+)?server/i,
      /here'?s?\s+(?:the\s+)?(?:json|config|configuration)\s+for\s+.*mcp/i,
      /\{[^}]*["']?(?:name|command|url)["']?\s*:/i,  // Looks like MCP JSON config
      /https?:\/\/github\.com\/[^\s]+/i  // GitHub URLs
    ]

    const isMCPServerRequest = mcpServerPatterns.some(pattern => pattern.test(messageContent))
    const githubUrlMatch = messageContent.match(/https?:\/\/github\.com\/[^\s]+/i)
    let isGitHubAnalysis = false

    if (isMCPServerRequest) {
      console.log('[Chat API] Detected MCP server request')

      // Handle GitHub URLs specially
      if (githubUrlMatch) {
        const githubUrl = githubUrlMatch[0]
        console.log('[Chat API] Processing GitHub URL:', githubUrl)

        // Natural language request with GitHub URL
        const serverManager = MCPServerManager.getInstance()
        const nlResult = await MCPServerIntelligence.processNaturalLanguageRequest(messageContent, serverManager)

        if (nlResult.action === 'add' && nlResult.suggestion) {
          // Successfully analyzed GitHub repo
          const config = await MCPConfigManager.loadConfig() || { servers: [] as any[] }

          const serverConfig: any = {
            name: nlResult.suggestion.name,
            transportType: nlResult.suggestion.transportType
          }

          if (nlResult.suggestion.transportType === 'stdio') {
            serverConfig.command = nlResult.suggestion.command
            if (nlResult.suggestion.args) serverConfig.args = nlResult.suggestion.args
            if (nlResult.suggestion.env) serverConfig.env = nlResult.suggestion.env
          } else {
            serverConfig.url = nlResult.suggestion.url
            if (nlResult.suggestion.apiKey) serverConfig.apiKey = nlResult.suggestion.apiKey
          }

          if (!config.servers) config.servers = []

          const existingIndex = config.servers.findIndex((s: any) => s.name === serverConfig.name)
          if (existingIndex >= 0) {
            config.servers[existingIndex] = serverConfig
          } else {
            config.servers.push(serverConfig)
          }

          await MCPConfigManager.saveConfig(config.servers)

          finalMessageContent = `## ðŸŽ‰ Successfully analyzed and added MCP server!

**Server**: ${nlResult.suggestion.name}
**Source**: ${githubUrl}
**Confidence**: ${(nlResult.suggestion.confidence * 100).toFixed(0)}%
${nlResult.suggestion.description ? `**Description**: ${nlResult.suggestion.description}` : ''}

**Configuration**:
\`\`\`json
${JSON.stringify(serverConfig, null, 2)}
\`\`\`

The server has been added and is ready to use. Would you like me to connect to it and show you the available tools?`
        } else {
          // Need to search for more information
          // Extract repo info from URL for better search
          const urlParts = githubUrl.match(/github\.com\/([^\/]+)\/([^\/]+)(?:\/tree\/[^\/]+)?(?:\/(.+))?/)
          const repoPath = urlParts ? `${urlParts[1]}/${urlParts[2]}` : 'repository'
          const serverPath = urlParts?.[3] || ''

          // Get available tools to find a web search tool
          const toolsCtx = await MCPToolsContext.getAvailableTools()
          const searchTool = toolsCtx.tools.find(t =>
            t.toolName.toLowerCase().includes('web_search') ||
            t.toolName.toLowerCase().includes('search')
          )

          if (searchTool) {
            finalMessageContent = `I'm analyzing the GitHub repository at ${githubUrl} to find the MCP server configuration. Let me search for more details.

[TOOL_CALL]
{
  "tool": "${searchTool.toolName}",
  "server": "${searchTool.serverName}",
  "arguments": {
    "query": "${githubUrl} MCP server installation npm install npx command configuration Model Context Protocol setup"
  }
}
[/TOOL_CALL]

${MCPGitHubPrompts.githubAnalysisFollowup}

After receiving the search results, I will:
1. Extract the installation command (npx or npm install)
2. Determine the correct package name
3. Identify any required environment variables
4. Build a complete MCP server configuration
5. Add it to your system automatically

Repository: ${repoPath}
${serverPath ? `Server path: ${serverPath}` : ''}`
          } else {
            // No search tool available
            finalMessageContent = `I need to analyze the GitHub repository at ${githubUrl}, but no search tools are currently available.

To enable GitHub repository analysis, please add a search tool like context7:
1. Click the + button in the MCP panel
2. Add context7 server with your API key
3. Then I'll be able to search for the MCP server configuration

Repository: ${repoPath}
${serverPath ? `Server path: ${serverPath}` : ''}`
          }
          isGitHubAnalysis = true
        }
      } else {
        // Check if it contains JSON configuration
        const jsonMatch = messageContent.match(/\{[\s\S]*\}|\[[\s\S]*\]/)
        if (jsonMatch) {
          // User provided JSON configuration
          const jsonInput = jsonMatch[0]
          const analysis = await MCPServerIntelligence.analyzeAndCorrectJSON(jsonInput)

          if (analysis.correctedJSON) {
            // Add the corrected server configuration
            const config = await MCPConfigManager.loadConfig() || { servers: [] as any[] }

            if (!config.servers) config.servers = []

            // Handle different JSON formats
            const serversToAdd = Array.isArray(analysis.correctedJSON)
              ? analysis.correctedJSON
              : analysis.correctedJSON.mcpServers
                ? Object.entries(analysis.correctedJSON.mcpServers).map(([name, cfg]: [string, any]) => ({ name, ...cfg }))
                : [analysis.correctedJSON]

            for (const server of serversToAdd) {
              const existingIndex = config.servers.findIndex((s: any) => s.name === server.name)
              if (existingIndex >= 0) {
                config.servers[existingIndex] = server
              } else {
                config.servers.push(server)
              }
            }

            await MCPConfigManager.saveConfig(config.servers)

            // Modify the message to inform about the addition
            const addedServers = serversToAdd.map((s: any) => s.name).join(', ')
            finalMessageContent = `I've analyzed and added the following MCP server(s): ${addedServers}. ` +
              (analysis.suggestions ? `Notes: ${analysis.suggestions.join('. ')}. ` : '') +
              `The server(s) are now available for use. Would you like me to test them or show you what tools they provide?`
          } else {
            // Could not parse or correct the JSON
            finalMessageContent = `I couldn't parse the MCP server configuration. ` +
              (analysis.errors ? `Errors: ${analysis.errors.join('. ')}. ` : '') +
              `Please provide a valid JSON configuration or tell me which MCP server you'd like to add (e.g., "add the filesystem MCP server").`
          }
        } else {
          // Natural language request
          const serverManager = MCPServerManager.getInstance()
          const nlResult = await MCPServerIntelligence.processNaturalLanguageRequest(messageContent, serverManager)

          if (nlResult.action === 'add' && nlResult.suggestion && nlResult.suggestion.confidence > 0.5) {
            // Add the suggested server
            const config = await MCPConfigManager.loadConfig() || { servers: [] as any[] }

            const serverConfig: any = {
              name: nlResult.suggestion.name,
              transportType: nlResult.suggestion.transportType
            }

            if (nlResult.suggestion.transportType === 'stdio') {
              serverConfig.command = nlResult.suggestion.command
              if (nlResult.suggestion.args) serverConfig.args = nlResult.suggestion.args
              if (nlResult.suggestion.env) serverConfig.env = nlResult.suggestion.env
            } else {
              serverConfig.url = nlResult.suggestion.url
              if (nlResult.suggestion.apiKey) serverConfig.apiKey = nlResult.suggestion.apiKey
            }

            if (!config.servers) config.servers = []

            const existingIndex = config.servers.findIndex((s: any) => s.name === serverConfig.name)
            if (existingIndex >= 0) {
              config.servers[existingIndex] = serverConfig
            } else {
              config.servers.push(serverConfig)
            }

            await MCPConfigManager.saveConfig(config.servers)

            finalMessageContent = `I've added the ${nlResult.serverName} MCP server with ${(nlResult.suggestion.confidence * 100).toFixed(0)}% confidence. ` +
              (nlResult.suggestion.description ? `This server provides: ${nlResult.suggestion.description}. ` : '') +
              `The server is now available for use. Would you like me to connect to it and show you what tools it provides?`
          } else if (nlResult.action === 'add' && !nlResult.suggestion) {
            // Need to search for the server configuration
            // Get available tools to find a web search tool
            const toolsCtx = await MCPToolsContext.getAvailableTools()
            const searchTool = toolsCtx.tools.find(t =>
              t.toolName.toLowerCase().includes('web_search') ||
              t.toolName.toLowerCase().includes('search')
            )

            if (searchTool) {
              finalMessageContent = `I'll help you add the ${nlResult.serverName || 'requested'} MCP server. Let me search for its configuration using available tools.

[TOOL_CALL]
{
  "tool": "${searchTool.toolName}",
  "server": "${searchTool.serverName}",
  "arguments": {
    "query": "MCP server ${nlResult.serverName} configuration JSON Model Context Protocol setup"
  }
}
[/TOOL_CALL]`
            } else {
              finalMessageContent = `I'd like to help you add the ${nlResult.serverName || 'requested'} MCP server, but I need a search tool to find its configuration.

Please add a search tool like context7 first:
1. Click the + button in the MCP panel
2. Add context7 server with your API key
3. Then I'll be able to search for the server configuration`
            }
          } else if (nlResult.action === 'list') {
            const config = await MCPConfigManager.loadConfig() || { servers: [] as any[] }
            const servers = config.servers || []

            if (servers.length === 0) {
              finalMessageContent = "You don't have any MCP servers configured yet. Would you like me to help you add some? Popular options include filesystem, github, postgres, and more."
            } else {
              const serverList = servers.map((s: any) => `- ${s.name} (${s.transportType || 'stdio'})`).join('\n')
              finalMessageContent = `Here are your configured MCP servers:\n\n${serverList}\n\nWould you like to add more servers or connect to any of these?`
            }
          } else {
            finalMessageContent = nlResult.message
          }
        }
      }
    }

    // Initialize tool orchestrator
    const orchestrator = new ToolOrchestrator();
    
    // Build orchestration context
    const orchestrationContext: OrchestrationContext = {
      hasFiles: !!(fileUri || multipleFiles?.length),
      fileTypes: multipleFiles?.map(f => f.mimeType) || (fileMimeType ? [fileMimeType] : []),
      conversationHistory: messages.map(m => m.content),
      userPreferences: {
        preferredImageModel: imageGenerationModel,
        preferredTTSVoice: undefined // Could be extracted from user settings
      }
    };
    
    // Orchestrate tool selection
    const orchestrationResult = await orchestrator.orchestrate(messageContent, orchestrationContext);
    console.log('[Chat API] Tool orchestration result:', orchestrationResult);
    
    // Storage for tool execution results
    let videoGenerationData: any = null
    let ttsGenerationData: any = null
    let imageEditingData: any = null
    let imageGenerationData: any = null
    
    // Handle tool execution based on orchestration result
    if (orchestrationResult.shouldExecute && !isMCPServerRequest) {
      console.log(`[Chat API] Executing tool: ${orchestrationResult.toolName}`);
      
      switch (orchestrationResult.toolId) {
        case 'dia-tts': {
          // Handle multi-speaker TTS
          console.log('[Chat API] Executing multi-speaker TTS via orchestrator');
          ttsGenerationData = await handleMultiSpeakerTTS(messageContent, messages, orchestrationResult.parameters);
          if (ttsGenerationData) {
            finalMessageContent = generateMultiSpeakerResponse(ttsGenerationData);
            
            // Return TTS response immediately without generating additional AI response
            const encoder = new TextEncoder();
            const stream = new ReadableStream({
              async start(controller) {
                try {
                  // Send the TTS response
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(finalMessageContent)}\n`));
                  
                  // Inject TTS generation data
                  if (ttsGenerationData) {
                    const ttsMarkerStart = '\n\n[TTS_GENERATION_STARTED]\n';
                    const ttsMarkerData = JSON.stringify(ttsGenerationData, null, 2);
                    const ttsMarkerEnd = '\n[/TTS_GENERATION_STARTED]';
                    
                    // Send complete TTS data as a single chunk to avoid parsing issues
                    const completeTTSBlock = ttsMarkerStart + ttsMarkerData + ttsMarkerEnd;
                    controller.enqueue(encoder.encode(`0:${JSON.stringify(completeTTSBlock)}\n`));
                  }
                  
                  // Send finish message
                  controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`));
                } catch (error) {
                  console.error("[Chat API] TTS streaming error:", error);
                  controller.enqueue(encoder.encode(`e:{"error":${JSON.stringify(error instanceof Error ? error.message : 'Unknown error')}}\n`));
                } finally {
                  controller.close();
                }
              }
            });
            
            return new Response(stream, {
              headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
              },
            });
          }
          break;
        }
        
        case 'wavespeed-tts': {
          // Handle WaveSpeed TTS
          console.log('[Chat API] Executing WaveSpeed TTS via orchestrator');
          
          // Check if this is incorrectly triggered for video/file analysis
          if (fileMimeType?.startsWith("video/") || 
              multipleFiles?.some(f => f.mimeType?.startsWith("video/")) ||
              (orchestrationResult.parameters?.text && 
               /reverse\s*engineer|analyze.*video|production.*breakdown/i.test(orchestrationResult.parameters.text))) {
            
            // Return error response for incorrect TTS trigger
            const encoder = new TextEncoder();
            const stream = new ReadableStream({
              async start(controller) {
                const errorMessage = fileMimeType?.startsWith("video/") 
                  ? "I see you have a video file attached. For video analysis, please use the 'Analyze & Transcribe' option instead of 'Reverse Engineer'. This will provide a comprehensive analysis with timestamps."
                  : "It looks like you're trying to analyze content rather than generate speech. Please try the 'Analyze' option for file analysis.";
                
                controller.enqueue(encoder.encode(`0:${JSON.stringify(errorMessage)}\n`));
                controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`));
                controller.close();
              }
            });
            
            return new Response(stream, {
              headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
              },
            });
          }
          
          // For now, let valid TTS requests fall through to legacy TTS detection
          // In the future, implement proper WaveSpeed handler here
          break;
        }
        
        case 'elevenlabs-tts': {
          // Handle single-speaker TTS
          console.log('[Chat API] Executing single-speaker TTS via orchestrator');
          ttsGenerationData = await handleSingleSpeakerTTS(messageContent, messages, orchestrationResult.parameters);
          if (ttsGenerationData) {
            finalMessageContent = generateSingleSpeakerResponse(ttsGenerationData);
            
            // Return TTS response immediately without generating additional AI response
            const encoder = new TextEncoder();
            const stream = new ReadableStream({
              async start(controller) {
                try {
                  // Send the TTS response
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(finalMessageContent)}\n`));
                  
                  // Inject TTS generation data
                  if (ttsGenerationData) {
                    const ttsMarkerStart = '\n\n[TTS_GENERATION_STARTED]\n';
                    const ttsMarkerData = JSON.stringify(ttsGenerationData, null, 2);
                    const ttsMarkerEnd = '\n[/TTS_GENERATION_STARTED]';
                    
                    // Send complete TTS data as a single chunk to avoid parsing issues
                    const completeTTSBlock = ttsMarkerStart + ttsMarkerData + ttsMarkerEnd;
                    controller.enqueue(encoder.encode(`0:${JSON.stringify(completeTTSBlock)}\n`));
                  }
                  
                  // Send finish message
                  controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`));
                } catch (error) {
                  console.error("[Chat API] TTS streaming error:", error);
                  controller.enqueue(encoder.encode(`e:{"error":${JSON.stringify(error instanceof Error ? error.message : 'Unknown error')}}\n`));
                } finally {
                  controller.close();
                }
              }
            });
            
            return new Response(stream, {
              headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
              },
            });
          }
          break;
        }
        
        case 'image-generation': {
          // Handle image generation
          console.log('[Chat API] Executing image generation via orchestrator');
          const imageRequest = {
            type: 'text-to-image' as const,
            prompt: orchestrationResult.parameters.prompt || messageContent,
            model: orchestrationResult.parameters.model || imageGenerationModel || 'gpt-image-1',
            quality: orchestrationResult.parameters.quality || 'standard',
            style: orchestrationResult.parameters.style || 'natural',
            size: orchestrationResult.parameters.aspectRatio || '1:1'
          };
          
          if (imageSettings) {
            // Apply user settings if not overridden
            imageRequest.size = imageRequest.size || imageSettings.size;
            imageRequest.style = imageRequest.style || imageSettings.style;
            imageRequest.quality = imageRequest.quality || imageSettings.quality;
          }
          
          imageGenerationData = await handleImageGeneration(imageRequest);
          if (imageGenerationData?.success) {
            finalMessageContent = ImageGenerationHandler.generateResponse(imageRequest);
          }
          break;
        }
        
        case 'video-generation': {
          // Handle video generation
          console.log('[Chat API] Executing video generation via orchestrator');
          const videoRequest = {
            type: fileUri ? 'image-to-video' as const : 'text-to-video' as const,
            prompt: orchestrationResult.parameters.prompt || messageContent,
            duration: orchestrationResult.parameters.duration || 5,
            aspectRatio: orchestrationResult.parameters.aspectRatio || '16:9',
            model: orchestrationResult.parameters.model || 'standard',
            imageUri: fileUri
          };
          
          videoGenerationData = await handleVideoGeneration(videoRequest);
          if (videoGenerationData) {
            finalMessageContent = VideoGenerationHandler.generateResponse(videoRequest);
          }
          break;
        }
        
        case 'image-editing': {
          // Handle image editing
          console.log('[Chat API] Executing image editing via orchestrator');
          const editRequest = {
            type: 'edit' as const,
            prompt: orchestrationResult.parameters.editInstructions || messageContent,
            imageUri: fileUri,
            model: imageEditingModel || 'gpt-image-1',
            quality: 'hd' as const,
            style: 'natural' as const,
            size: '1024x1024' as const
          };
          
          imageEditingData = await handleImageEditing(editRequest);
          if (imageEditingData?.success) {
            finalMessageContent = ImageEditingHandler.generateResponse(editRequest);
          }
          break;
        }
        
        default:
          console.log(`[Chat API] Tool ${orchestrationResult.toolId} not implemented in direct execution`);
      }
    } else if (!orchestrationResult.shouldExecute && orchestrationResult.requiresConfirmation) {
      // Add orchestration explanation to the message for Gemini to handle
      finalMessageContent = orchestrationResult.explanation + "\n\n" + finalMessageContent;
    }
    
    // Fall back to legacy detection for backwards compatibility
    if (!orchestrationResult.shouldExecute || orchestrationResult.toolId === 'none') {
      // Enhanced detection with file context - now with async aspect ratio detection
      const videoRequest = await VideoGenerationHandler.detectVideoRequest(messageContent, fileUri, fileMimeType)
      const imageEditRequest = ImageEditingHandler.detectEditRequest(messageContent, fileUri, fileMimeType)

      // Override the model if user selected one in settings
      if (imageEditRequest && imageEditingModel) {
        imageEditRequest.model = imageEditingModel as any;
      }

    // Priority: Image editing takes precedence over video generation if both are detected
    if (imageEditRequest && !isMCPServerRequest) {
      console.log('[Chat API] Detected image editing request:', imageEditRequest)

      try {
        // Check if we have OPENAI_API_KEY
        if (!process.env.OPENAI_API_KEY) {
          finalMessageContent = `I understand you want to edit an image, but the image editing feature is not configured yet.

To enable image editing, you need to:
1. Sign up for an OpenAI account at https://platform.openai.com
2. Get your API key from https://platform.openai.com/api-keys
3. Add it to your .env.local file as OPENAI_API_KEY

Once configured, I'll be able to edit images using GPT-Image-1 inpainting.`
        } else {
          // Convert Google AI File Manager URI to a format we can use
          let imageUrl = imageEditRequest.imageUri

          // For now, we'll need to handle the URI conversion
          // TODO: Implement proper image URL conversion
          if (ImageUrlConverter.isGoogleAIFileUri(imageUrl || '')) {
            // Use a temporary approach - return instructions for now
            finalMessageContent = `I can see you want to edit your uploaded image, but there's a technical limitation with accessing images from Google AI File Manager for editing.

**Workaround for now:**
1. Download your image locally
2. Re-upload it using a direct image URL or file upload
3. Then I can edit it using GPT-Image-1

**What you wanted to do:** "${imageEditRequest.prompt}"

I'm working on implementing seamless image editing from uploads. For now, please try the workaround above.`
          } else {
            // Trigger actual image editing
            const editResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || `http://localhost:${process.env.PORT || '3001'}`}/api/edit-image`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                imageUrl: imageUrl,
                prompt: imageEditRequest.prompt,
                quality: imageEditRequest.quality,
                style: imageEditRequest.style,
                size: imageEditRequest.size
              })
            })

            if (editResponse.ok) {
              const result = await editResponse.json()
              console.log('[IMAGE EDIT] Image editing API response:', result)

              // Store image editing data to inject after AI response
              imageEditingData = {
                success: true,
                images: result.images,
                metadata: result.metadata,
                prompt: imageEditRequest.prompt
              }

              finalMessageContent = ImageEditingHandler.generateResponse(imageEditRequest)
            } else {
              const error = await editResponse.text()
              finalMessageContent = `I encountered an error while trying to edit the image:

${error}

Please check your OpenAI API configuration and try again.`
            }
        }
          }
        } catch (error) {
          console.error('Image editing error:', error)
          finalMessageContent = `I encountered an error while trying to edit the image:

${error instanceof Error ? error.message : 'Unknown error'}

Please make sure the image editing API is properly configured.`
        }
    } else if (videoRequest && !isMCPServerRequest) {
      console.log('[Chat API] Detected video generation request:', videoRequest)
    console.log('[Chat API] Current URL origin:', req.url ? new URL(req.url).origin : 'unknown')
    console.log('[Chat API] Will call video generation API...')

      // Check if this is an image-to-video request (user mentioned animating an image)
      if (videoRequest.type === 'image-to-video') {
        try {
          // Check if we have REPLICATE_API_KEY
          if (!process.env.REPLICATE_API_KEY) {
            finalMessageContent = `I understand you want to animate an image, but the video generation feature is not configured yet.

To enable video generation, you need to:
1. Sign up for a Replicate account at https://replicate.com
2. Get your API token from https://replicate.com/account/api-tokens
3. Add it to your .env.local file as REPLICATE_API_KEY

Once configured, I'll be able to animate images using Kling v1.6 models.`
          } else {
            // For image-to-video, we can try using the Google AI File Manager URI directly
            // Many external APIs can handle these URIs, and if not, we'll get a clear error
            let startImageUrl = videoRequest.imageUri

            console.log('[VIDEO] Processing image-to-video request with URI:', startImageUrl);

            // Try video generation with the Google AI File Manager URI
            try {
              // Trigger actual video generation with image
              const videoGenResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || `http://localhost:${process.env.PORT || '3001'}`}/api/generate-video`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  prompt: videoRequest.prompt,
                  duration: videoRequest.duration,
                  aspectRatio: videoRequest.aspectRatio,
                  model: videoRequest.model,
                  negativePrompt: videoRequest.negativePrompt,
                  startImage: startImageUrl,
                  backend: videoRequest.backend || 'replicate',
                  tier: videoRequest.tier || 'fast'
                })
              })

              if (videoGenResponse.ok) {
        console.log('[VIDEO] Video generation API call successful');
                const result = await videoGenResponse.json()
                console.log('[VIDEO] Image-to-video generation API response:', result)

                // Store video generation data to inject after AI response
                console.log('[VIDEO API] Storing image-to-video generation data for injection')
                videoGenerationData = {
                  id: result.id,
                  url: result.output || '',
                  status: result.status === 'succeeded' ? 'succeeded' : 'generating',
                  prompt: videoRequest.prompt,
                  duration: videoRequest.duration,
                  aspectRatio: videoRequest.aspectRatio,
                  model: videoRequest.model,
                  sourceImage: startImageUrl
                }

                finalMessageContent = VideoGenerationHandler.generateResponse(videoRequest)
              } else {
                const error = await videoGenResponse.text()
                console.error('[VIDEO] Image-to-video generation failed:', error)

                // If the direct approach failed, provide helpful feedback
                finalMessageContent = `I encountered an issue while trying to animate your uploaded image:

**Error:** ${error}

This might be because the image format needs to be converted. Let me try to help you work around this:

**Alternative approaches:**
1. Try uploading the image in a different format (JPEG, PNG)
2. Use a direct image URL if you have one
3. The image processing system may need a moment to fully process your upload

**What you wanted:** "${videoRequest.prompt}"
**Duration:** ${videoRequest.duration} seconds
**Model:** Kling v1.6 ${videoRequest.model === 'pro' ? 'Pro' : 'Standard'}

Please try uploading the image again or use a direct image URL if available.`
              }
            } catch (networkError) {
              console.error('[VIDEO] Network error during image-to-video:', networkError)
              finalMessageContent = `I encountered a network error while trying to animate your image:

**Error:** ${networkError instanceof Error ? networkError.message : 'Unknown network error'}

Please try again in a moment, or ensure your connection is stable.`
            }
          }
        } catch (error) {
          console.error('Image-to-video generation error:', error)
          finalMessageContent = `I encountered an error while trying to animate the image:

${error instanceof Error ? error.message : 'Unknown error'}

Please make sure the video generation API is properly configured.`
        }
      } else {
        // For text-to-video, trigger actual video generation
        try {
          // Check if we have REPLICATE_API_KEY
          if (!process.env.REPLICATE_API_KEY) {
            finalMessageContent = `I understand you want to generate a video, but the video generation feature is not configured yet.

To enable video generation, you need to:
1. Sign up for a Replicate account at https://replicate.com
2. Get your API token from https://replicate.com/account/api-tokens
3. Add it to your .env.local file as REPLICATE_API_KEY

Once configured, I'll be able to generate videos using Kling v1.6 models.`
          } else {
            // Trigger actual video generation
            const videoGenResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || `http://localhost:${process.env.PORT || '3001'}`}/api/generate-video`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                prompt: videoRequest.prompt,
                duration: videoRequest.duration,
                aspectRatio: videoRequest.aspectRatio,
                model: videoRequest.model,
                negativePrompt: videoRequest.negativePrompt,
                backend: videoRequest.backend || 'replicate',
                tier: videoRequest.tier || 'fast'
              })
            })

            if (videoGenResponse.ok) {
              const result = await videoGenResponse.json()
              console.log('[VIDEO] Video generation API response:', result)

              // Store video generation data to inject after AI response
              console.log('[VIDEO API] Storing video generation data for injection')
              videoGenerationData = {
                id: result.id,
                url: result.output || '',
                status: result.status === 'succeeded' ? 'succeeded' : 'generating',
                prompt: videoRequest.prompt,
                duration: videoRequest.duration,
                aspectRatio: videoRequest.aspectRatio,
                model: videoRequest.model
              }

              // Check if we got a direct video URL or a prediction ID
              if (result.status === 'succeeded' && result.output) {
                // Direct generation succeeded
                console.log('[VIDEO] Direct video generation succeeded with URL:', result.output)
                finalMessageContent = `I've generated a ${videoRequest.duration}-second video of "${videoRequest.prompt}" for you! ðŸŽ¬

**Video Generation Complete**
- Model: Kling v1.6 ${videoRequest.model === 'pro' ? 'Pro' : 'Standard'}
- Duration: ${videoRequest.duration} seconds
- Aspect Ratio: ${videoRequest.aspectRatio}
- Status: Completed

The video is ready and should appear in the **Video** tab on the right.`
              } else {
                // Prediction-based generation (needs polling)
                finalMessageContent = `I'm generating a ${videoRequest.duration}-second video of "${videoRequest.prompt}" for you! ðŸŽ¬

**Video Generation Started**
- ID: ${result.id}
- Model: Kling v1.6 ${videoRequest.model === 'pro' ? 'Pro' : 'Standard'}
- Duration: ${videoRequest.duration} seconds
- Aspect Ratio: ${videoRequest.aspectRatio}
- Status: ${result.status || 'generating'}

The video is being generated and will appear in the **Video** tab on the right. Kling v1.6 typically takes 5-8 minutes.`
              }
            } else {
              const error = await videoGenResponse.text()
              finalMessageContent = `I encountered an error while trying to generate the video:

${error}

Please check your Replicate API configuration and try again.`
            }
          }
        } catch (error) {
          console.error('Video generation error:', error)
          finalMessageContent = `I encountered an error while trying to generate the video:

${error instanceof Error ? error.message : 'Unknown error'}

Please make sure the video generation API is properly configured.`
        }
      }
    } else {
      // Check for standalone image generation request (not video, not editing)
      const imageRequest = ImageGenerationHandler.detectImageRequest(messageContent, imageGenerationModel)
      
      if (imageRequest && !isMCPServerRequest) {
        // Apply user's image settings from request body if not explicitly overridden in message
        if (imageSettings) {
          // Only override if not explicitly mentioned in the message
          const lowerMessage = messageContent.toLowerCase()
          
          // Use settings size unless user explicitly mentions size in message
          if (!lowerMessage.includes('landscape') && !lowerMessage.includes('portrait') && 
              !lowerMessage.includes('square') && !lowerMessage.includes('wide') && 
              !lowerMessage.includes('tall') && !lowerMessage.includes('horizontal') &&
              !lowerMessage.includes('vertical')) {
            imageRequest.size = imageSettings.size
          }
          
          // Use settings style unless user explicitly mentions style in message
          if (!lowerMessage.includes('natural') && !lowerMessage.includes('realistic') && 
              !lowerMessage.includes('photorealistic') && !lowerMessage.includes('vivid')) {
            imageRequest.style = imageSettings.style
          }
          
          // Use settings quality unless user explicitly mentions quality in message
          if (!lowerMessage.includes('standard') && !lowerMessage.includes('hd') && 
              !lowerMessage.includes('high') && !lowerMessage.includes('quality')) {
            imageRequest.quality = imageSettings.quality
          }
        }
        
        console.log('[Chat API] Detected image generation request:', imageRequest)
        
        try {
          // Determine which API key to check based on model
          const needsOpenAI = imageRequest.model === 'gpt-image-1'
          const needsReplicate = imageRequest.model === 'flux-kontext-pro' || imageRequest.model === 'flux-kontext-max'
          const needsWaveSpeed = imageRequest.model === 'flux-dev-ultra-fast'
          
          // Check if required API key is configured
          if (needsOpenAI && !process.env.OPENAI_API_KEY) {
            finalMessageContent = `I understand you want to generate an HD quality image, but GPT-Image-1 is not configured yet.

To enable HD image generation, you need to:
1. Sign up for an OpenAI account at https://platform.openai.com
2. Get your API key from https://platform.openai.com/api-keys
3. Add it to your .env.local file as OPENAI_API_KEY

Alternatively, I can generate a standard quality image using WaveSpeed if you have WAVESPEED_API_KEY configured.`
          } else if (needsReplicate && !process.env.REPLICATE_API_KEY) {
            finalMessageContent = `I understand you want to generate an image with ${imageRequest.model}, but Replicate is not configured yet.

To enable this model, you need to:
1. Sign up for a Replicate account at https://replicate.com
2. Get your API token from https://replicate.com/account/api-tokens
3. Add it to your .env.local file as REPLICATE_API_KEY

Alternatively, I can generate an image using other models if you have their API keys configured.`
          } else if (needsWaveSpeed && !process.env.WAVESPEED_API_KEY) {
            finalMessageContent = `I understand you want to generate an image, but WaveSpeed is not configured yet.

To enable image generation, you need to:
1. Sign up for a WaveSpeed account at https://wavespeed.ai
2. Get your API key
3. Add it to your .env.local file as WAVESPEED_API_KEY

Alternatively, I can generate an HD quality image using GPT-Image-1 if you have OPENAI_API_KEY configured.`
          } else {
            // Trigger actual image generation
            const imageGenResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || `http://localhost:${process.env.PORT || '3001'}`}/api/generate-image`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                prompt: imageRequest.prompt,
                originalPrompt: messageContent,
                model: imageRequest.model,
                quality: imageRequest.quality,
                style: imageRequest.style,
                size: imageRequest.size
              })
            })

            if (imageGenResponse.ok) {
              const result = await imageGenResponse.json()
              console.log('[IMAGE] Image generation API response:', result)

              // Store image generation data to inject after AI response
              imageGenerationData = {
                success: true,
                images: result.images,
                metadata: result.metadata,
                prompt: imageRequest.prompt
              }

              finalMessageContent = ImageGenerationHandler.generateResponse(imageRequest)
            } else {
              const error = await imageGenResponse.text()
              finalMessageContent = `I encountered an error while trying to generate the image:

${error}

Please check your API configuration and try again.`
            }
          }
        } catch (error) {
          console.error('Image generation error:', error)
          finalMessageContent = `I encountered an error while trying to generate the image:

${error instanceof Error ? error.message : 'Unknown error'}

Please make sure the image generation API is properly configured.`
        }
      }
    }

    // Check for multi-speaker TTS command first (more specific)
    // Only proceed if orchestrator didn't exclude TTS tools
    const orchestratorExcludedTTS = orchestrationResult.toolId !== 'none' && 
      !['wavespeed-tts', 'dia-tts', 'elevenlabs-tts'].includes(orchestrationResult.toolId) &&
      orchestrationResult.confidence > 60;
    
    if (containsMultiSpeakerTTSCommand(messageContent) && !videoGenerationData && !imageEditingData && !imageGenerationData && !orchestratorExcludedTTS) {
      console.log('[Chat API] Multi-speaker TTS command detected');
      
      try {
        // Process multi-speaker TTS
        const result = await processMultiSpeakerTTS(messageContent, messages);
        
        if (result.success && result.audioBase64) {
          // Create TTS data object for frontend
          const ttsData = {
            text: result.script || '',
            script: result.script,
            audioBase64: result.audioBase64,
            mimeType: 'audio/wav',
            timestamp: Date.now(),
            voiceId: 'dia-multi',
            voiceName: `Dia Multi-Speaker (${result.metadata?.speakers || 2} speakers)`,
            isMultiSpeaker: true,
            metadata: result.metadata
          };
          
          // Generate response text
          let responseText = `ðŸŽ­ I've generated a multi-speaker dialogue with ${result.metadata?.speakers || 2} distinct voices!\n\n`;
          responseText += `**Style:** ${result.metadata?.style || 'custom'}\n`;
          responseText += `**Speakers:** ${result.metadata?.speakers || 2}\n\n`;
          responseText += `The audio is ready in the **Audio** tab. Each speaker has a unique voice that remains consistent throughout the dialogue.\n\n`;
          responseText += `**Generated Script:**\n\`\`\`\n${result.script}\n\`\`\``;
          
          finalMessageContent = responseText;
          
          // Stream the response with TTS data
          const encoder = new TextEncoder();
          const stream = new ReadableStream({
            async start(controller) {
              try {
                // Send the text response
                controller.enqueue(encoder.encode(`0:${JSON.stringify(finalMessageContent)}\n`));
                
                // Send TTS data
                if (ttsData) {
                  controller.enqueue(encoder.encode(`d:${JSON.stringify({ ttsData })}\n`));
                }
                
                // Send finish message
                controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`));
              } catch (error) {
                console.error('[Chat API] Streaming error:', error);
                controller.enqueue(encoder.encode(`e:{"error":${JSON.stringify(error instanceof Error ? error.message : 'Unknown error')}}\n`));
              } finally {
                controller.close();
              }
            }
          });
          
          return new Response(stream, {
            headers: {
              'Content-Type': 'text/event-stream',
              'Cache-Control': 'no-cache',
              'Connection': 'keep-alive',
            },
          });
        } else {
          // Handle error
          finalMessageContent = `I encountered an error while generating the multi-speaker audio:\n\n${result.error || 'Unknown error'}\n\nPlease make sure the text has proper speaker tags like [S1] and [S2], or try using a simpler command like: /tts multi "topic" speakers:2`;
        }
      } catch (error) {
        console.error('[Chat API] Multi-speaker TTS error:', error);
        finalMessageContent = `I encountered an error while generating the multi-speaker audio:\n\n${error instanceof Error ? error.message : 'Unknown error'}\n\nPlease try again.`;
      }
    }
    // Check if this is a regular TTS request
    // Only proceed if orchestrator didn't exclude TTS tools
    else if (containsTTSCommand(messageContent) && !videoGenerationData && !imageEditingData && !imageGenerationData && !orchestratorExcludedTTS) {
      console.log('[Chat API] Detected TTS request')
      
      try {
        // Extract TTS content
        const ttsContent = extractTTSContent(messageContent)
        
        // Enhanced script extraction for voice over requests
        let scriptText = ttsContent.text
        const isVoiceOverRequest = messageContent.toLowerCase().includes('script') || 
                                  messageContent.toLowerCase().includes('voice over') ||
                                  messageContent.toLowerCase().includes('this into audio') ||
                                  messageContent.toLowerCase().includes('turn this into') ||
                                  messageContent.toLowerCase().includes('make this audio')
        
        if ((!scriptText || scriptText.trim().length < 5) && isVoiceOverRequest) {
          console.log('[TTS] Voice over request detected, searching for script in previous messages...')
          
          // Look for a script in the previous assistant messages
          const previousMessages = messages.slice(-8) // Check more messages
          for (let i = previousMessages.length - 1; i >= 0; i--) {
            const msg = previousMessages[i]
            if (msg.role === 'assistant' && msg.content.length > 100) {
              
              // Enhanced script detection patterns - more inclusive for plain text
              const hasScriptMarkers = (
                msg.content.includes('###') || // Markdown headers
                msg.content.includes('Speaker') || // Speaker labels
                msg.content.includes('[') && msg.content.includes(']') || // Audio tags
                msg.content.includes('**') || // Bold text (common in scripts)
                msg.content.match(/\(Voice:/i) || // Voice specifications
                msg.content.match(/\*\*.*?\*\*/g) || // Bold formatting
                msg.content.includes('(Voice:') || // Voice tags
                msg.content.includes('[clears throat]') || // Action tags
                msg.content.includes('[sighs]') || // Action tags
                msg.content.includes('So...') || // Common script starts
                msg.content.match(/^[A-Z][a-z\s,]*:/) || // Speaker patterns
                msg.content.match(/^From the/i) || // Common narrative starts
                msg.content.match(/\.\.\./g) || // Ellipsis (common in scripts)
                (msg.content.length > 200 && !msg.content.toLowerCase().includes('here') && !msg.content.toLowerCase().includes('this is'))
              )
              
              // For voice-over requests, be much more liberal - any substantial content could be a script
              const isLikelyScript = hasScriptMarkers || 
                                   (msg.content.length > 150 && !msg.content.includes('I\'ll') && !msg.content.includes('I can'))
              
              if (isLikelyScript) {
                console.log('[TTS] Found potential script in message:', msg.content.substring(0, 100) + '...')
                
                // Extract the actual script content - be less aggressive for plain text
                let extractedScript = msg.content
                
                // Only remove obvious instruction patterns, preserve the main content
                extractedScript = extractedScript
                  .replace(/^Here['']?s? (?:a|an|the) .*?(?:script|story|narrative).*?:\s*/i, '') // Remove intro only
                  .replace(/\*\*(?:Script|Story|Narrative).*?\*\*\s*/i, '') // Remove headers only
                  .replace(/(?:The audio|You can listen|This audio will be).*$/i, '') // Remove trailing instructions only
                  .replace(/\n\*+\n*$/, '') // Remove trailing asterisks
                  .replace(/^TTS:\s*/i, '') // Remove TTS: prefix
                  .replace(/^Read this aloud:\s*/i, '') // Remove Read this aloud: prefix
                  .replace(/^Okay,.*?TTS:\s*/i, '') // Remove any preamble before TTS:
                  .trim()
                
                // For plain narrative text, don't filter too aggressively
                if (!hasScriptMarkers && extractedScript.length > 100) {
                  console.log('[TTS] Plain text script detected, minimal filtering applied')
                }
                
                // If it's still long enough, use it (lowered threshold for better detection)
                if (extractedScript.length > 30) {
                  scriptText = extractedScript
                  console.log('[TTS] Successfully extracted script, length:', scriptText.length)
                  console.log('[TTS] Script preview:', scriptText.substring(0, 200) + '...')
                  break
                }
              }
            }
          }
          
          // Final fallback - if we still don't have good script text, refuse the request
          if (!scriptText || scriptText.trim().length < 10) {
            console.log('[TTS] Could not find suitable script in previous messages')
            
            // Return an error response instead of trying to generate TTS
            const encoder = new TextEncoder()
            const stream = new ReadableStream({
              async start(controller) {
                const errorMessage = "I couldn't find a script in the previous messages to convert to audio. Please share the text you'd like me to read aloud, or ask me to create a new script first."
                controller.enqueue(encoder.encode(`0:${JSON.stringify(errorMessage)}\n`))
                controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`))
                controller.close()
              }
            })
            
            return new Response(stream, {
              headers: {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
              },
            })
          }
        }
        
        // Generate TTS
        const ttsResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || `http://localhost:${process.env.PORT || '3001'}`}/api/generate-speech`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: scriptText || ttsContent.text,
            voiceName: ttsContent.voiceName,
            style: ttsContent.style,
            multiSpeaker: ttsContent.multiSpeaker,
            generateScript: ttsContent.generateScript,
            contextPrompt: messageContent
          })
        })

        if (ttsResponse.ok) {
          const result = await ttsResponse.json()
          console.log('[TTS] TTS generation successful')

          // Store TTS generation data to inject after AI response
          ttsGenerationData = {
            id: result.id || `tts-${Date.now()}`,
            text: result.text,
            script: result.script,
            audioBase64: result.audioBase64,
            mimeType: result.mimeType || 'audio/mpeg',
            timestamp: result.timestamp || Date.now(),
            voiceId: result.voiceId,
            voiceName: result.voiceName,
            isMultiSpeaker: result.isMultiSpeaker || false
          }

          let responseText = ""
          
          if (isVoiceOverRequest) {
            // For voice-over requests, keep the response minimal and focused
            responseText = `âœ… Audio voice-over generated using the ${result.voiceName} voice.\n\nThe audio is ready in the **Audio** tab.`
          } else if (result.isMultiSpeaker) {
            responseText = `I've generated a multi-speaker dialogue with ${result.voiceName} ðŸŽ­\n\nThe audio features multiple voices in conversation and is ready in the **Audio** tab.`
          } else if (result.script) {
            responseText = `I've created an enhanced audio narration using the ${result.voiceName} voice ðŸŽ™ï¸\n\nThe script has been enhanced with expressive audio tags for natural delivery. The audio is ready in the **Audio** tab.`
          } else {
            responseText = `I've generated the audio using the ${result.voiceName} voice ðŸ”Š\n\nThe audio is ready and should appear in the **Audio** tab on the right. You can play it directly or download it for later use.`
          }
          
          // Only show the script for new script generation, not for voice-over conversion
          if (result.script && ttsContent.generateScript && !isVoiceOverRequest) {
            responseText += `\n\n**Generated Script:**\n\`\`\`\n${result.script}\n\`\`\``
          }
          
          finalMessageContent = responseText
          
          // Return TTS response immediately without generating additional AI response
          const encoder = new TextEncoder()
          const stream = new ReadableStream({
            async start(controller) {
              try {
                // Send the TTS response
                controller.enqueue(encoder.encode(`0:${JSON.stringify(finalMessageContent)}\n`))
                
                // Inject TTS generation data
                if (ttsGenerationData) {
                  const ttsMarkerStart = '\n\n[TTS_GENERATION_STARTED]\n'
                  const ttsMarkerData = JSON.stringify(ttsGenerationData, null, 2)
                  const ttsMarkerEnd = '\n[/TTS_GENERATION_STARTED]'
                  
                  // Send complete TTS data as a single chunk to avoid parsing issues
                  const completeTTSBlock = ttsMarkerStart + ttsMarkerData + ttsMarkerEnd
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(completeTTSBlock)}\n`))
                }
                
                // Send finish message
                controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`))
              } catch (error) {
                console.error("TTS streaming error:", error)
                controller.enqueue(encoder.encode(`e:{"error":${JSON.stringify(error instanceof Error ? error.message : 'Unknown error')}}
`))
              } finally {
                controller.close()
              }
            }
          })
          
          return new Response(stream, {
            headers: {
              'Content-Type': 'text/event-stream',
              'Cache-Control': 'no-cache',
              'Connection': 'keep-alive',
            },
          })
        
        } else {
          const error = await ttsResponse.text()
          console.error('[TTS] TTS generation failed:', error)
          finalMessageContent = `I encountered an error while generating the speech audio:

${error}

Please make sure the text-to-speech API is properly configured.`
        }
      } catch (error) {
        console.error('[TTS] TTS generation error:', error)
        finalMessageContent = `I encountered an error while generating the speech audio:

${error instanceof Error ? error.message : 'Unknown error'}

Please try again in a moment.`
      }
    }

    // Check if we should skip Gemini execution
    const toolsExecuted = videoGenerationData || imageEditingData || ttsGenerationData || imageGenerationData
    const shouldSkipGemini = toolsExecuted && !isMCPServerRequest
    
    if (shouldSkipGemini) {
      console.log('[Chat API] Tool already executed, skipping Gemini chat')
      // For tools that haven't returned yet, we need to ensure they have responses
      if (!finalMessageContent) {
        finalMessageContent = "Tool execution completed."
      }
    }

    // Get MCP tools context
    console.log('[Chat API] Getting MCP tools context...')
    const toolsContext = await MCPToolsContext.getAvailableTools()

    // Add GitHub analysis instructions if we're searching for MCP config
    if (githubUrlMatch && isMCPServerRequest && isGitHubAnalysis) {
      const githubPrompt = `
${MCPGitHubPrompts.searchResultAnalysis}

When you receive search results about the GitHub repository, analyze them to find:
1. The correct NPM package name or installation command
2. How to run the server (npx, npm install -g, etc.)
3. Any required configuration or environment variables
4. Whether it uses stdio (default) or http transport

After receiving the search results, I will automatically analyze them and configure the server for you.`

      finalMessageContent = githubPrompt + '\n\n' + finalMessageContent
    }
    console.log('[Chat API] Tools context:', {
      toolsCount: toolsContext.tools.length,
      hasSystemPrompt: !!toolsContext.systemPrompt,
      systemPromptLength: toolsContext.systemPrompt.length
    })
    if (toolsContext.tools.length > 0) {
      console.log('[Chat API] Available tools:', toolsContext.tools.map(t => `${t.serverName}:${t.toolName}`))
    }
    // Only add system prompt if this is NOT image-only generation and we're continuing to Gemini
    // For image-only generation, we use predetermined responses and don't need system prompts
    if (toolsContext.systemPrompt && !imageGenerationData && !shouldSkipGemini) {
      finalMessageContent = toolsContext.systemPrompt + "\n\n" + finalMessageContent
    }

    // Note: Transcription is now handled directly by Gemini for better context understanding
    // The old Whisper transcription code is commented out below:
    /*
    // If we have transcription data for video/audio, include it as context
    if (transcription && transcription.text && (fileMimeType?.startsWith("video/") || fileMimeType?.startsWith("audio/"))) {
      const duration = transcription.duration ? ` (Duration: ${Math.floor(transcription.duration / 60)}:${Math.floor(transcription.duration % 60).toString().padStart(2, '0')})` : ''
      const language = transcription.language ? ` [Language: ${transcription.language}]` : ''

      finalMessageContent += `\n\nTranscription of audio track${language}${duration}:\n"${transcription.text}"\n\nPlease incorporate this transcription into your complete analysis.`

      console.log(`Including transcription in analysis - Length: ${transcription.text.length} chars`)
    }
    */


    contentParts.push({ text: finalMessageContent })

    // If we should skip Gemini, create a direct response
    if (shouldSkipGemini && finalMessageContent) {
      console.log('[Chat API] Creating direct response for tool execution')
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        async start(controller) {
          try {
            // Send the tool response
            controller.enqueue(encoder.encode(`0:${JSON.stringify(finalMessageContent)}\n`))
            
            // Inject any tool-specific data
            if (videoGenerationData) {
              const videoMarkerStart = '\n\n[VIDEO_GENERATION_STARTED]\n'
              const videoMarkerData = JSON.stringify(videoGenerationData, null, 2)
              const videoMarkerEnd = '\n[/VIDEO_GENERATION_STARTED]'
              controller.enqueue(encoder.encode(`0:${JSON.stringify(videoMarkerStart + videoMarkerData + videoMarkerEnd)}\n`))
            }
            
            if (imageEditingData) {
              const imageMarkerStart = '\n\n[IMAGE_EDITING_COMPLETED]\n'
              const imageMarkerData = JSON.stringify(imageEditingData, null, 2)
              const imageMarkerEnd = '\n[/IMAGE_EDITING_COMPLETED]'
              controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerStart + imageMarkerData + imageMarkerEnd)}\n`))
            }
            
            if (imageGenerationData) {
              const imageMarkerStart = '\n\n[IMAGE_GENERATION_COMPLETED]\n'
              const imageMarkerData = JSON.stringify(imageGenerationData, null, 2)
              const imageMarkerEnd = '\n[/IMAGE_GENERATION_COMPLETED]'
              controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerStart + imageMarkerData + imageMarkerEnd)}\n`))
            }
            
            // Send finish message
            controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`))
          } catch (error) {
            console.error('[Chat API] Direct response streaming error:', error)
            controller.enqueue(encoder.encode(`e:{"error":${JSON.stringify(error instanceof Error ? error.message : 'Unknown error')}}\n`))
          } finally {
            controller.close()
          }
        }
      })
      
      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        },
      })
    }

    // Use AI SDK streamText instead of manual streaming
    if (model === "gemini-2.5-flash-preview-05-20" || model === "gemini-2.5-pro-preview-05-06" || model === "gemini-2.5-pro-preview-06-05" || model === "gemini-2.0-flash-exp") {
      // For video generation, image editing, image generation, and TTS, we'll handle them specially
      if (videoGenerationData || imageEditingData || ttsGenerationData || imageGenerationData) {
        // For image generation only, skip Gemini and use our response directly
        let result = null
        if (imageGenerationData && !videoGenerationData && !imageEditingData && !ttsGenerationData) {
          console.log('[Chat API] Image generation only - using direct response without Gemini')
          // Don't send to Gemini, we already have our response
        } else {
          // Send message and get streaming response using manual approach for special data injection
          result = await chat.sendMessageStream(contentParts)
        }

        // Create proper streaming response with injected data
        const encoder = new TextEncoder()
        let responseBuffer = ''

        const stream = new ReadableStream({
          async start(controller) {
            try {
              // Collect the response
              if (result) {
                for await (const chunk of result.stream) {
                  const text = chunk.text()
                  responseBuffer += text
                }
                console.log('[Chat API] Gemini response length:', responseBuffer.length)
              }
              
              // For image generation only, use our predetermined response
              if (imageGenerationData && !videoGenerationData && !imageEditingData && !ttsGenerationData) {
                console.log('[Chat API] Using predetermined response for image generation')
                responseBuffer = imageGenerationData.success 
                  ? ImageGenerationHandler.generateResponse({
                      type: 'text-to-image',
                      prompt: imageGenerationData.prompt,
                      model: imageGenerationData.metadata?.model || 'gpt-image-1',
                      quality: imageGenerationData.metadata?.quality as any || 'hd'
                    })
                  : 'I encountered an error while generating the image.'
                
                // Ensure response is clean and doesn't contain model documentation
                console.log('[Chat API] Clean image response length:', responseBuffer.length)
              }

              // Send the response in proper Server-Sent Events format
              console.log('[Chat API] Sending response, length:', responseBuffer.length)
              const responseChunks = responseBuffer.split(' ')
              for (const chunk of responseChunks) {
                if (chunk.trim()) {
                  // Use proper AI SDK format: 0:"content"
                  const escapedChunk = JSON.stringify(chunk + ' ')
                  controller.enqueue(encoder.encode(`0:${escapedChunk}\n`))
                }
              }

              // Inject video generation data if available
              if (videoGenerationData) {
                // Split the video marker into smaller chunks to avoid parsing issues
                const videoMarkerStart = '\n\n[VIDEO_GENERATION_STARTED]\n'
                const videoMarkerData = JSON.stringify(videoGenerationData, null, 2)
                const videoMarkerEnd = '\n[/VIDEO_GENERATION_STARTED]'
                
                // Send each part separately
                controller.enqueue(encoder.encode(`0:${JSON.stringify(videoMarkerStart)}\n`))
                
                // Split the JSON data into smaller chunks
                const chunkSize = 100
                for (let i = 0; i < videoMarkerData.length; i += chunkSize) {
                  const chunk = videoMarkerData.slice(i, i + chunkSize)
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(chunk)}\n`))
                }
                
                controller.enqueue(encoder.encode(`0:${JSON.stringify(videoMarkerEnd)}\n`))
              }

              // Inject image editing data if available
              if (imageEditingData) {
                // Split the image marker into smaller chunks to avoid parsing issues
                const imageMarkerStart = '\n\n[IMAGE_EDITING_COMPLETED]\n'
                const imageMarkerData = JSON.stringify(imageEditingData, null, 2)
                const imageMarkerEnd = '\n[/IMAGE_EDITING_COMPLETED]'
                
                // Send each part separately
                controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerStart)}\n`))
                
                // Split the JSON data into smaller chunks
                const chunkSize = 100
                for (let i = 0; i < imageMarkerData.length; i += chunkSize) {
                  const chunk = imageMarkerData.slice(i, i + chunkSize)
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(chunk)}\n`))
                }
                
                controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerEnd)}\n`))
              }

              // Inject TTS generation data if available
              if (ttsGenerationData) {
                const ttsMarkerStart = '\n\n[TTS_GENERATION_STARTED]\n'
                const ttsMarkerData = JSON.stringify(ttsGenerationData, null, 2)
                const ttsMarkerEnd = '\n[/TTS_GENERATION_STARTED]'
                
                // Send complete TTS data as a single chunk to avoid parsing issues
                const completeTTSBlock = ttsMarkerStart + ttsMarkerData + ttsMarkerEnd
                controller.enqueue(encoder.encode(`0:${JSON.stringify(completeTTSBlock)}\n`))
              }

              // Inject image generation data if available
              if (imageGenerationData) {
                const imageMarkerStart = '\n\n[IMAGE_GENERATION_COMPLETED]\n'
                const imageMarkerData = JSON.stringify(imageGenerationData, null, 2)
                const imageMarkerEnd = '\n[/IMAGE_GENERATION_COMPLETED]'
                
                // Send each part separately to avoid parsing issues
                controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerStart)}\n`))
                
                // Split the JSON data into smaller chunks
                // Use larger chunk size for efficiency, especially with base64 images
                const chunkSize = 8192 // 8KB chunks
                console.log(`[Chat API] Sending image data in ${Math.ceil(imageMarkerData.length / chunkSize)} chunks`)
                for (let i = 0; i < imageMarkerData.length; i += chunkSize) {
                  const chunk = imageMarkerData.slice(i, i + chunkSize)
                  controller.enqueue(encoder.encode(`0:${JSON.stringify(chunk)}\n`))
                }
                
                controller.enqueue(encoder.encode(`0:${JSON.stringify(imageMarkerEnd)}\n`))
              }

              // Send finish message
              controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`))
            } catch (error) {
              console.error("Streaming error:", error)
              const errorMessage = error instanceof Error ? error.message : "Unknown error"
              const escapedError = JSON.stringify(errorMessage)
              controller.enqueue(encoder.encode(`3:${escapedError}\n`))
            } finally {
              controller.close()
            }
          },
        })

        return new Response(stream, {
          headers: {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
          },
        })
      } else {
        // For regular streaming without special data injection, use simpler approach
        const result = await chat.sendMessageStream(contentParts)

        const encoder = new TextEncoder()
        const stream = new ReadableStream({
          async start(controller) {
            try {
              for await (const chunk of result.stream) {
                const text = chunk.text()
                if (text) {
                  // Use proper AI SDK format: 0:"content"
                  const escapedText = JSON.stringify(text)
                  controller.enqueue(encoder.encode(`0:${escapedText}\n`))
                }
              }
              controller.enqueue(encoder.encode(`d:{"finishReason":"stop"}\n`))
            } catch (error) {
              console.error("Streaming error:", error)
              const errorMessage = error instanceof Error ? error.message : "Unknown error"
              const escapedError = JSON.stringify(errorMessage)
              controller.enqueue(encoder.encode(`3:${escapedError}\n`))
            } finally {
              controller.close()
            }
          },
        })

        return new Response(stream, {
          headers: {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
          },
        })
      }
    } else {
      // For non-Gemini models, return a simple response
      return new Response(
        JSON.stringify({ error: "Unsupported model: " + model }),
        {
          status: 400,
          headers: {
            'Content-Type': 'application/json',
          },
        }
      )
    }
  } catch (error) {
    console.error("Chat API Error:", error)

    // Return error in data stream format
    const errorMessage = error instanceof Error ? error.message : "Unknown error"
    return new Response(
      `3:${JSON.stringify(errorMessage)}\n`,
      {
        status: 200, // Keep 200 for data stream compatibility
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
        }
      }
    )
  }
}
